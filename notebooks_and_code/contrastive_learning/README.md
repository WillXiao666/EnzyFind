# ConPLex-Style Contrastive Learning for Enzyme-Metabolite Interaction Prediction

This module implements a contrastive learning approach inspired by ConPLex for predicting enzyme-metabolite interactions. The method maps both metabolites and enzymes to a shared latent space and uses contrastive learning with triplet loss to improve interaction prediction.

## üéØ Overview

### Problem Statement
Traditional enzyme-metabolite interaction prediction relies on concatenating molecular features and using classical machine learning models like XGBoost. This approach may not capture the complex relationships between enzymes and metabolites in a shared semantic space.

### Solution: ConPLex-Style Contrastive Learning
Our approach:
1. **Shared Latent Space**: Projects metabolite and enzyme features to the same latent space using separate MLP networks
2. **Contrastive Learning**: Uses triplet loss to pull interacting pairs closer and push non-interacting pairs apart
3. **Two-Stage Training**: First trains with binary classification loss, then adds contrastive learning
4. **Combined Objective**: Balances classification accuracy with embedding quality

## üèóÔ∏è Architecture

```
Metabolite Features (Unimol/GNN) ‚Üí MLP Projector ‚Üí Latent Space (128D)
                                                      ‚Üì
                                                  Concatenate ‚Üí Classifier ‚Üí Interaction Probability
                                                      ‚Üë
Enzyme Features (ESM-1b)        ‚Üí MLP Projector ‚Üí Latent Space (128D)
```

### Key Components

1. **MLPProjector**: Projects input features to shared latent space
2. **ConPLexModel**: Main model combining projectors and classifier
3. **TripletLoss**: Implements margin-based contrastive loss
4. **CombinedLoss**: Balances classification and contrastive objectives

## üìä Training Strategy

### Stage 1: Classification Training
- **Objective**: Binary cross-entropy loss
- **Purpose**: Learn basic interaction patterns
- **Duration**: 10 epochs (configurable)

### Stage 2: Contrastive Training  
- **Objective**: Combined classification + triplet loss
- **Purpose**: Refine embeddings in latent space
- **Triplet Construction**: (enzyme, positive_metabolite, negative_metabolite)
- **Margin Decay**: Gradually reduce triplet margin following ConPLex methodology
- **Duration**: 20 epochs (configurable)

## üöÄ Quick Start

### Installation

```bash
# Install required dependencies
pip install torch numpy pandas scikit-learn matplotlib seaborn tqdm pyyaml

# Optional: For logging and visualization
pip install wandb
```

### Basic Usage

```python
from contrastive_learning.train_conplex import main

# Run training with configuration file
trainer, results = main("config.yaml")
```

### Configuration

Edit `config.yaml` to customize training:

```yaml
# Model architecture
model:
  latent_dim: 128
  hidden_dims: [256, 128]
  dropout_rate: 0.2

# Training settings
training:
  batch_size: 32
  learning_rate: 0.001
  classification_epochs: 10
  contrastive_epochs: 20
  margin: 1.0
```

## üíª Usage Examples

### 1. Train a New Model

```python
# Train from scratch
python contrastive_learning/train_conplex.py --config config.yaml
```

### 2. Run Demonstration

```python
# Quick demo with synthetic data
python contrastive_learning/demo.py
```

### 3. Evaluate Trained Model

```python
from contrastive_learning.evaluation import ConPLexEvaluator

# Load and evaluate model
evaluator = ConPLexEvaluator(trained_trainer)
results = evaluator.comprehensive_evaluation(test_loader)
```

### 4. Make Predictions

```python
# Predict interactions for new enzyme-metabolite pairs
predictions = trainer.predict(metabolite_features, enzyme_features)

# Get latent embeddings
met_emb, enz_emb = trainer.get_embeddings(metabolite_features, enzyme_features)
```

## üìà Performance Metrics

The model is evaluated using comprehensive metrics:

- **ROC-AUC**: Area under ROC curve
- **PR-AUC**: Area under Precision-Recall curve  
- **Matthews Correlation Coefficient (MCC)**: Balanced metric for imbalanced data
- **Accuracy, Precision, Recall, F1-Score**: Standard classification metrics
- **Embedding Quality**: Cosine similarity analysis in latent space

## üî¨ Methodology Details

### Triplet Construction Strategy

1. **Positive Pairs**: Known enzyme-metabolite interactions
2. **Negative Sampling**: 
   - For enzyme anchor: Sample metabolites that don't interact with the enzyme
   - For metabolite anchor: Sample enzymes that don't interact with the metabolite
3. **Balanced Sampling**: 5 negatives per positive (configurable)

### Loss Function

```
Total Loss = Œ± √ó Classification Loss + Œ≤ √ó Contrastive Loss

Classification Loss = BCE(predictions, labels)
Contrastive Loss = max(0, d(anchor, positive) - d(anchor, negative) + margin)
```

Where:
- Œ±, Œ≤: Balancing weights (default: 1.0, 1.0)
- d(): Distance function (Euclidean or cosine)
- margin: Triplet margin (starts at 1.0, decays by 0.95/epoch)

### Data Augmentation

- **Negative Sampling**: Ensures diverse negative examples
- **Balanced Batches**: Maintains positive/negative ratio within batches
- **Embedding Normalization**: L2 normalization for stable contrastive learning

## üìÅ File Structure

```
contrastive_learning/
‚îú‚îÄ‚îÄ __init__.py              # Module initialization
‚îú‚îÄ‚îÄ model.py                 # ConPLex model architecture
‚îú‚îÄ‚îÄ trainer.py               # Training pipeline implementation
‚îú‚îÄ‚îÄ data_loader.py           # Data loading and triplet generation
‚îú‚îÄ‚îÄ evaluation.py            # Evaluation and visualization tools
‚îú‚îÄ‚îÄ train_conplex.py         # Main training script
‚îú‚îÄ‚îÄ demo.py                  # Demonstration script
‚îú‚îÄ‚îÄ config.yaml              # Configuration file
‚îî‚îÄ‚îÄ README.md               # This documentation
```

## üîß Configuration Options

### Model Parameters
- `latent_dim`: Shared latent space dimension (default: 128)
- `hidden_dims`: MLP hidden layer sizes (default: [256, 128])
- `dropout_rate`: Dropout probability (default: 0.2)
- `activation`: Activation function (default: 'relu')

### Training Parameters
- `learning_rate`: Optimizer learning rate (default: 0.001)
- `weight_decay`: L2 regularization (default: 0.0001)
- `classification_weight`: Weight for classification loss (default: 1.0)
- `contrastive_weight`: Weight for contrastive loss (default: 1.0)
- `margin`: Initial triplet margin (default: 1.0)
- `margin_decay`: Margin decay factor (default: 0.95)

### Data Parameters
- `batch_size`: Training batch size (default: 32)
- `val_split`: Validation split ratio (default: 0.2)
- `negative_sampling_ratio`: Negatives per positive (default: 5)

## üìä Expected Results

### Performance Improvements
Based on ConPLex methodology, expected improvements over baseline:
- **ROC-AUC**: +2-5% improvement
- **PR-AUC**: +3-7% improvement (more significant for imbalanced data)
- **MCC**: +5-10% improvement in balanced accuracy
- **Embedding Quality**: Better separation of interacting vs non-interacting pairs

### Training Time
- **Classification Stage**: ~5-10 minutes (10 epochs)
- **Contrastive Stage**: ~15-30 minutes (20 epochs)
- **Total**: ~20-40 minutes on GPU for typical dataset

## üîç Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce `batch_size` in config.yaml
   - Reduce `latent_dim` or `hidden_dims`

2. **Poor Convergence**
   - Adjust learning rate (try 0.0001 or 0.01)
   - Increase `classification_epochs` for better initialization
   - Check data balance and negative sampling ratio

3. **Low Contrastive Learning Performance**
   - Ensure sufficient negative samples
   - Adjust `contrastive_weight` (try 0.5 or 2.0)
   - Verify triplet construction logic

### Performance Tips

1. **For Large Datasets**
   - Use gradient accumulation
   - Implement efficient negative sampling
   - Consider distributed training

2. **For Imbalanced Data**
   - Increase `negative_sampling_ratio`
   - Use class weights in classification loss
   - Focus on PR-AUC over ROC-AUC

## üìö References and Related Work

### Key Papers
1. **ConPLex**: "Learning to Rank Compounds for Structure-Activity Relationships"
2. **Triplet Loss**: "FaceNet: A Unified Embedding for Face Recognition and Clustering"
3. **Contrastive Learning**: "A Simple Framework for Contrastive Learning of Visual Representations"

### GitHub Repositories for Learning

1. **ConPLex Implementation**
   ```
   https://github.com/samsledje/ConPLex
   ```
   - Original ConPLex implementation
   - Protein-ligand interaction prediction
   - Contrastive learning with protein language models

2. **Triplet Loss PyTorch**
   ```
   https://github.com/adambielski/siamese-triplet
   ```
   - Clean triplet loss implementation
   - Siamese networks and contrastive learning
   - Good examples of triplet mining strategies

3. **Protein-Molecule Interaction**
   ```
   https://github.com/kexinhuang12345/DeepPurpose
   ```
   - Drug-target interaction prediction
   - Multiple molecular representation methods
   - Comprehensive benchmarking

4. **ESM Protein Embeddings**
   ```
   https://github.com/facebookresearch/esm
   ```
   - Evolutionary Scale Modeling for proteins
   - Pre-trained protein language models
   - Example usage with downstream tasks

5. **UniMol Molecular Representation**
   ```
   https://github.com/deepmodeling/Uni-Mol
   ```
   - Universal molecular representation
   - 3D molecular pretraining
   - Integration with downstream tasks

6. **Contrastive Learning Framework**
   ```
   https://github.com/PyTorchLightning/lightning-bolts
   ```
   - PyTorch Lightning implementations
   - Self-supervised learning methods
   - Contrastive learning utilities

### Learning Resources

1. **Contrastive Learning Tutorial**
   - Understanding triplet loss and margin-based learning
   - Hard negative mining strategies
   - Batch construction for contrastive learning

2. **Protein-Molecule Interaction Survey**
   - Recent advances in computational methods
   - Benchmark datasets and evaluation metrics
   - Comparison of different approaches

## üìÑ License

This implementation is provided for research and educational purposes. Please cite appropriate papers when using this code in your research.

## ü§ù Contributing

Contributions are welcome! Please consider:
- Code improvements and optimizations
- Additional evaluation metrics
- Support for different molecular representations
- Documentation improvements

## üìû Contact

For questions and support, please:
1. Check the troubleshooting section
2. Review the demonstration script
3. Open an issue with detailed error information